"""
Trending Topics Detection - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º
–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ trending –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
"""
import sys
sys.path.insert(0, '/home/valstan/SETKA')

import logging
from typing import List, Dict, Set
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import re
from sqlalchemy import select, and_
from sqlalchemy.ext.asyncio import AsyncSession

from database.connection import AsyncSessionLocal
from database.models import Post, Region

logger = logging.getLogger(__name__)


class TrendingTopicsDetector:
    """
    –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ trending —Ç–µ–º across —Ä–µ–≥–∏–æ–Ω–æ–≤
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤
    - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø–æ—Ö–æ–∂–∏—Ö –ø–æ—Å—Ç–æ–≤
    - Cross-region analysis
    """
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–Ω–µ —É—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ)
    STOP_WORDS = {
        '–≤', '–Ω–∞', '–∏', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '—É', '–æ', '–æ–±',
        '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '—Ç–æ', '–≤—Å–µ', '–≤—Å—ë', '–≤—ã', '–º—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏',
        '—á—Ç–æ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏',
        '–±—É–¥–µ—Ç', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–±—ã—Ç—å'
    }
    
    async def detect_trending_topics(
        self,
        hours: int = 24,
        min_posts: int = 3,
        min_regions: int = 2
    ) -> List[Dict]:
        """
        –ù–∞–π—Ç–∏ trending —Ç–µ–º—ã
        
        Args:
            hours: –ü–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥)
            min_posts: –ú–∏–Ω–∏–º—É–º –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è trending
            min_regions: –ú–∏–Ω–∏–º—É–º —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è cross-region trending
            
        Returns:
            List trending —Ç–µ–º
        """
        logger.info(f"Detecting trending topics (last {hours}h, min {min_posts} posts, {min_regions} regions)")
        
        # –ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã
        posts = await self._get_recent_posts(hours)
        
        if len(posts) < min_posts:
            logger.info(f"Not enough posts ({len(posts)})")
            return []
        
        logger.info(f"Analyzing {len(posts)} posts...")
        
        # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤
        keywords_by_post = {}
        all_keywords = Counter()
        
        for post in posts:
            keywords = self._extract_keywords(post.text)
            keywords_by_post[post.id] = keywords
            all_keywords.update(keywords)
        
        # –ù–∞–π—Ç–∏ —Ç–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (potential topics)
        top_keywords = [word for word, count in all_keywords.most_common(50) if count >= min_posts]
        
        logger.info(f"Found {len(top_keywords)} potential topics")
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞, –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç—ã –∏ —Ä–µ–≥–∏–æ–Ω—ã
        trending_topics = []
        
        for keyword in top_keywords:
            # –ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã —Å —ç—Ç–∏–º keyword
            posts_with_keyword = [
                post for post in posts
                if keyword in keywords_by_post.get(post.id, set())
            ]
            
            # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–≥–∏–æ–Ω—ã
            regions = set(post.region_id for post in posts_with_keyword)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ trending
            if len(posts_with_keyword) >= min_posts and len(regions) >= min_regions:
                # –í—ã—á–∏—Å–ª–∏—Ç—å engagement
                total_engagement = sum(
                    post.views + post.likes * 2 + post.reposts * 5
                    for post in posts_with_keyword
                )
                
                trending_topics.append({
                    'keyword': keyword,
                    'post_count': len(posts_with_keyword),
                    'region_count': len(regions),
                    'regions': list(regions),
                    'total_engagement': total_engagement,
                    'avg_engagement': total_engagement / len(posts_with_keyword),
                    'sample_posts': [
                        {
                            'id': p.id,
                            'text': p.text[:100] if p.text else '',
                            'views': p.views
                        }
                        for p in sorted(posts_with_keyword, key=lambda x: x.views, reverse=True)[:3]
                    ]
                })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ engagement
        trending_topics.sort(key=lambda t: t['total_engagement'], reverse=True)
        
        logger.info(f"Found {len(trending_topics)} trending topics")
        
        return trending_topics
    
    async def _get_recent_posts(self, hours: int) -> List:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã"""
        async with AsyncSessionLocal() as session:
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            
            result = await session.execute(
                select(Post)
                .where(
                    and_(
                        Post.date_published >= cutoff_time,
                        Post.ai_analyzed == True,
                        Post.status != 'rejected'
                    )
                )
                .order_by(Post.date_published.desc())
            )
            
            posts = result.scalars().all()
            return list(posts)
    
    def _extract_keywords(self, text: str, min_length: int = 4) -> Set[str]:
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç
            min_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
            
        Returns:
            Set –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        """
        if not text:
            return set()
        
        # Lowercase –∏ –æ—á–∏—Å—Ç–∫–∞
        text = text.lower()
        
        # –£–¥–∞–ª–∏—Ç—å URL
        text = re.sub(r'http\S+|www.\S+', '', text)
        
        # –£–¥–∞–ª–∏—Ç—å —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ª–æ–≤–∞
        words = text.split()
        
        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å
        keywords = {
            word for word in words
            if len(word) >= min_length
            and word not in self.STOP_WORDS
            and not word.isdigit()
        }
        
        return keywords
    
    async def get_region_specific_trends(
        self,
        region_code: str,
        hours: int = 24,
        limit: int = 10
    ) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å trending —Ç–µ–º—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞
        
        Args:
            region_code: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞
            hours: –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
            limit: –ú–∞–∫—Å–∏–º—É–º —Ç–µ–º
            
        Returns:
            List trending —Ç–µ–º –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞
        """
        async with AsyncSessionLocal() as session:
            # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–≥–∏–æ–Ω
            result = await session.execute(
                select(Region).where(Region.code == region_code)
            )
            region = result.scalar_one_or_none()
            
            if not region:
                return []
            
            # –ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã —Ä–µ–≥–∏–æ–Ω–∞
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            
            result = await session.execute(
                select(Post)
                .where(
                    and_(
                        Post.region_id == region.id,
                        Post.date_published >= cutoff_time,
                        Post.ai_analyzed == True
                    )
                )
            )
            
            posts = result.scalars().all()
            
            if not posts:
                return []
            
            # –ò–∑–≤–ª–µ—á—å keywords
            all_keywords = Counter()
            for post in posts:
                keywords = self._extract_keywords(post.text)
                all_keywords.update(keywords)
            
            # –¢–æ–ø keywords
            trending = []
            for keyword, count in all_keywords.most_common(limit):
                # –ù–∞–π—Ç–∏ –ø–æ—Å—Ç—ã —Å —ç—Ç–∏–º keyword
                posts_with_keyword = [
                    p for p in posts
                    if keyword in self._extract_keywords(p.text)
                ]
                
                total_engagement = sum(
                    p.views + p.likes * 2 + p.reposts * 5
                    for p in posts_with_keyword
                )
                
                trending.append({
                    'keyword': keyword,
                    'count': count,
                    'engagement': total_engagement,
                    'sample_post': posts_with_keyword[0].text[:100] if posts_with_keyword else ''
                })
            
            return trending


if __name__ == "__main__":
    import asyncio
    
    async def test():
        detector = TrendingTopicsDetector()
        
        print("="*60)
        print("üß™ Testing Trending Topics Detector")
        print("="*60)
        
        # Test 1: Detect trending (last 24h)
        print("\n1. Detecting trending topics (last 24 hours)...")
        topics = await detector.detect_trending_topics(hours=24, min_posts=2, min_regions=1)
        
        if topics:
            print(f"\n   Found {len(topics)} trending topics:")
            for i, topic in enumerate(topics[:5], 1):
                print(f"\n   {i}. \"{topic['keyword']}\"")
                print(f"      Posts: {topic['post_count']}, Regions: {topic['region_count']}")
                print(f"      Engagement: {topic['total_engagement']}")
        else:
            print("   No trending topics found")
        
        # Test 2: Region-specific trends
        print("\n2. Region-specific trends (mi)...")
        region_trends = await detector.get_region_specific_trends('mi', hours=72, limit=5)
        
        if region_trends:
            print(f"\n   Found {len(region_trends)} trending keywords:")
            for i, trend in enumerate(region_trends, 1):
                print(f"   {i}. \"{trend['keyword']}\" (count: {trend['count']}, engagement: {trend['engagement']})")
        else:
            print("   No trends found")
        
        print("\n‚úÖ Test completed!")
    
    asyncio.run(test())

