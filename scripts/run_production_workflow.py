#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Production Workflow –¥–ª—è SETKA

–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏:
1. VK –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
2. Filter Pipeline
3. AI –∞–Ω–∞–ª–∏–∑
4. Scoring
5. –ê–≥—Ä–µ–≥–∞—Ü–∏—è
6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""
import asyncio
import sys
import os
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/home/valstan/SETKA/logs/production_workflow.log')
    ]
)

logger = logging.getLogger(__name__)

from sqlalchemy import select, update, and_
from database.connection import AsyncSessionLocal
from database.models import Region, Community, Post, VKToken, Filter
from modules.vk_monitor.monitor import VKMonitor
from modules.module_activity_notifier import (
    notify_workflow_started,
    notify_workflow_completed,
    notify_region_processing,
    notify_publish_started,
    notify_publish_completed
)
from modules.filters import (
    FilterPipeline,
    StructuralDuplicateFilter,
    DateFilter,
    BlacklistIDFilter,
    TextDuplicateFilter,
    BlacklistWordFilter,
    TextLengthFilter,
    SpamPatternFilter,
    ViewsRequirementFilter,
    TextQualityFilter,
    RegionalRelevanceFilter,
    CategoryFilter
)
from modules.aggregation.aggregator import NewsAggregator
from modules.core.scoring import calculate_post_score
from modules.operation_tracking import (
    start_monitoring_operation, start_filtering_operation, 
    start_publishing_operation, update_operation_progress,
    end_operation_success, end_operation_error
)


class ProductionWorkflow:
    """
    Production workflow –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
    - VK Monitor: —Å–±–æ—Ä –ø–æ—Å—Ç–æ–≤
    - Filter Pipeline: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    - AI Analyzer: –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
    - Aggregation: —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤
    """
    
    def __init__(self):
        self.stats = {
            'start_time': datetime.now(),
            'regions_processed': 0,
            'posts_collected': 0,
            'posts_filtered': 0,
            'posts_accepted': 0,
            'errors': []
        }
    
    async def get_vk_tokens(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ VK —Ç–æ–∫–µ–Ω—ã –∏–∑ –ë–î"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(VKToken).where(VKToken.is_active == True)
            )
            tokens_objs = result.scalars().all()
            tokens = [t.token for t in tokens_objs if t.token]
            logger.info(f"Loaded {len(tokens)} VK tokens")
            return tokens
    
    async def load_filters(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –ë–î"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Filter).where(Filter.is_active == True)
            )
            filters = result.scalars().all()
            
            blacklist_words = []
            blacklist_ids = []
            
            for f in filters:
                if f.type == 'blacklist_word':
                    blacklist_words.append(f.pattern)
                elif f.type == 'blacklist_id':
                    try:
                        blacklist_ids.append(int(f.pattern))
                    except ValueError:
                        pass
            
            logger.info(f"Loaded {len(blacklist_words)} word filters, {len(blacklist_ids)} ID filters")
            
            return {
                'blacklist_words': blacklist_words,
                'blacklist_ids': blacklist_ids
            }
    
    async def create_filter_pipeline(
        self,
        region: Region,
        filters_data: Dict[str, List[str]]
    ) -> FilterPipeline:
        """
        –°–æ–∑–¥–∞—Ç—å Filter Pipeline –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞
        
        –ü–æ—Ä—è–¥–æ–∫ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏–∑ Postopus:
        1. –ë—ã—Å—Ç—Ä–∞—è –æ—Ç—Å–µ—á–∫–∞ (LIP, –¥–∞—Ç—ã, ID)
        2. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–¥–ª–∏–Ω–∞, –ø—Ä–æ—Å–º–æ—Ç—Ä—ã)
        3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è (—Ç–µ–∫—Å—Ç, –º–µ–¥–∏–∞)
        4. –ß–µ—Ä–Ω—ã–µ —Å–ø–∏—Å–∫–∏ (—Å–ª–æ–≤–∞, –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        5. –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        6. –ö–∞—á–µ—Å—Ç–≤–æ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Ç–µ–∫—Å—Ç)
        """
        pipeline = FilterPipeline([
            # –£—Ä–æ–≤–µ–Ω—å 1: –ë—ã—Å—Ç—Ä–∞—è –æ—Ç—Å–µ—á–∫–∞
            StructuralDuplicateFilter(),
            DateFilter(max_age_hours=72),
            BlacklistIDFilter(),  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑ –ë–î
            
            # –£—Ä–æ–≤–µ–Ω—å 2: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            TextLengthFilter(min_length=10, max_length=10000),
            ViewsRequirementFilter(min_views=0),
            
            # –£—Ä–æ–≤–µ–Ω—å 3: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            TextDuplicateFilter(check_full=True, check_core=True),
            
            # –£—Ä–æ–≤–µ–Ω—å 4: –ß–µ—Ä–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            BlacklistWordFilter(),  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑ –ë–î
            SpamPatternFilter(),
            
            # –£—Ä–æ–≤–µ–Ω—å 5: –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            RegionalRelevanceFilter(required_matches=1),  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑ –ë–î
            
            # –£—Ä–æ–≤–µ–Ω—å 6: –ö–∞—á–µ—Å—Ç–≤–æ
            CategoryFilter(allowed_categories=['novost', 'kultura', 'sport', 'proisshestvie']),
            TextQualityFilter(min_words=3),
        ])
        
        return pipeline
    
    async def process_region(
        self,
        region_code: str,
        vk_tokens: List[str],
        filters_data: Dict[str, List[str]],
        max_posts: int = 100
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ä–µ–≥–∏–æ–Ω
        
        Args:
            region_code: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ (mi, nolinsk, etc.)
            vk_tokens: VK —Ç–æ–∫–µ–Ω—ã
            filters_data: –î–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
            max_posts: –ú–∞–∫—Å–∏–º—É–º –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"üåç Processing region: {region_code}")
        logger.info(f"{'='*70}")
        
        # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–≥–∏–æ–Ω–∞
        notify_region_processing(region_code, "–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        region_stats = {
            'region_code': region_code,
            'posts_collected': 0,
            'posts_before_filter': 0,
            'posts_after_filter': 0,
            'posts_accepted': 0,
            'errors': []
        }
        
        try:
            async with AsyncSessionLocal() as session:
                # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–≥–∏–æ–Ω
                result = await session.execute(
                    select(Region).where(Region.code == region_code)
                )
                region = result.scalar_one_or_none()
                
                if not region:
                    error_msg = f"Region {region_code} not found"
                    logger.error(error_msg)
                    region_stats['errors'].append(error_msg)
                    return region_stats
                
                # –î–æ–±–∞–≤–ª—è–µ–º region_id –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                region_stats['region_id'] = region.id
                
                logger.info(f"üìç Region: {region.name} (ID: {region.id})")
                
                # 1. –ó–∞–ø—É—Å—Ç–∏—Ç—å VK –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                logger.info("\nüîç Step 1: VK Monitoring...")
                notify_region_processing(region_code, "VK –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
                
                # Start monitoring operation tracking
                monitoring_op_id = start_monitoring_operation(
                    region_code, 
                    0  # Will be updated after getting communities count
                )
                
                try:
                    monitor = VKMonitor(vk_tokens=vk_tokens)
                    scan_result = await monitor.scan_region(region_code)
                    
                    region_stats['posts_collected'] = scan_result.get('new_posts', 0)
                    logger.info(f"‚úÖ Collected {region_stats['posts_collected']} new posts")
                    
                    # Update operation progress
                    update_operation_progress(
                        monitoring_op_id,
                        progress=100,
                        current_step="completed",
                        details={"posts_collected": region_stats['posts_collected']}
                    )
                    
                except Exception as e:
                    end_operation_error(monitoring_op_id, str(e))
                    raise
                finally:
                    end_operation_success(monitoring_op_id, {
                        "posts_collected": region_stats['posts_collected']
                    })
                
                # 2. –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                logger.info("\nüîç Step 2: Loading posts for filtering...")
                notify_region_processing(region_code, "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
                
                # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞)
                recent_threshold = datetime.now() - timedelta(hours=24)
                
                posts_result = await session.execute(
                    select(Post).where(
                        and_(
                            Post.region_id == region.id,
                            Post.date_published >= recent_threshold,
                            Post.ai_analyzed == False  # –¢–æ–ª—å–∫–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                        )
                    ).limit(max_posts)
                )
                posts = list(posts_result.scalars().all())
                
                region_stats['posts_before_filter'] = len(posts)
                logger.info(f"üìä Loaded {len(posts)} posts for filtering")
                
                if not posts:
                    logger.info("‚ÑπÔ∏è No new posts to process")
                    return region_stats
                
                # 3. –°–æ–∑–¥–∞—Ç—å Filter Pipeline
                logger.info("\nüîç Step 3: Creating Filter Pipeline...")
                notify_region_processing(region_code, "–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä-–ø–∞–π–ø–ª–∞–π–Ω–∞")
                pipeline = await self.create_filter_pipeline(region, filters_data)
                
                # 4. –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
                logger.info("\nüîç Step 4: Applying filters...")
                notify_region_processing(region_code, "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤")
                
                # Start filtering operation tracking
                filtering_op_id = start_filtering_operation(region_code, len(posts))
                
                try:
                    context = {
                        'session': session,
                        'region': region,
                        'content_type': 'novost'
                    }
                    
                    filtered_posts, pipeline_result = await pipeline.process(posts, context)
                    
                    # Update operation progress
                    update_operation_progress(
                        filtering_op_id,
                        progress=100,
                        current_step="completed",
                        details={
                            "posts_before": len(posts),
                            "posts_after": len(filtered_posts),
                            "rejection_rate": f"{100 * (1 - len(filtered_posts) / max(len(posts), 1)):.1f}%"
                        }
                    )
                    
                except Exception as e:
                    end_operation_error(filtering_op_id, str(e))
                    raise
                finally:
                    end_operation_success(filtering_op_id, {
                        "posts_before": len(posts),
                        "posts_after": len(filtered_posts)
                    })
                
                region_stats['posts_after_filter'] = len(filtered_posts)
                region_stats['posts_accepted'] = len(filtered_posts)
                
                logger.info(f"‚úÖ Filtering complete:")
                logger.info(f"   Before: {len(posts)}")
                logger.info(f"   After:  {len(filtered_posts)}")
                logger.info(f"   Rejected: {len(posts) - len(filtered_posts)} ({100 * (1 - len(filtered_posts) / max(len(posts), 1)):.1f}%)")
                
                # 5. –û–±–Ω–æ–≤–∏—Ç—å scoring –∏ –ø–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                logger.info("\nüîç Step 5: Updating scores...")
                notify_region_processing(region_code, "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫")
                
                for post in filtered_posts:
                    # –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å score
                    post.ai_score = calculate_post_score(
                        views=post.views or 0,
                        likes=post.likes or 0,
                        reposts=post.reposts or 0,
                        comments=post.comments or 0,
                        posted_at=post.date_published,
                        source_priority=1.0,
                        ai_category_weight=0.8 if post.ai_category == 'novost' else 0.5
                    )
                    
                    # –ü–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
                    post.ai_analyzed = True
                    post.ai_analysis_date = datetime.now()
                
                await session.commit()
                
                logger.info(f"‚úÖ Updated scores for {len(filtered_posts)} posts")
                
                # 6. –°–æ–∑–¥–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ü–∏—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                if len(filtered_posts) >= 3:
                    logger.info("\nüîç Step 6: Creating aggregated digest...")
                    
                    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ ai_score
                    sorted_posts = sorted(filtered_posts, key=lambda p: p.ai_score or 0, reverse=True)
                    top_posts = sorted_posts[:5]
                    
                    aggregator = NewsAggregator(
                        max_posts_per_digest=5,
                        max_text_length=4000,
                        max_media_items=10
                    )
                    
                    digest = await aggregator.aggregate(
                        posts=top_posts,
                        title=f"üì∞ –ù–æ–≤–æ—Å—Ç–∏ | {region.name}",
                        hashtags=[f"#{region_code}", "#–Ω–æ–≤–æ—Å—Ç–∏"]
                    )
                    
                    if digest:
                        logger.info(f"‚úÖ Created digest with {len(digest.additional_posts) + 1} posts")
                        logger.info(f"   Total views: {digest.total_views}")
                        categories_str = ', '.join(filter(None, digest.categories))
                        logger.info(f"   Categories: {categories_str}")
                    else:
                        logger.info("‚ÑπÔ∏è Could not create digest")
                
                logger.info(f"\n‚úÖ Region {region_code} processing complete!")
                notify_region_processing(region_code, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                
        except Exception as e:
            error_msg = f"Error processing region {region_code}: {str(e)}"
            logger.error(error_msg, exc_info=True)
            region_stats['errors'].append(error_msg)
        
        return region_stats
    
    async def run(
        self,
        region_codes: Optional[List[str]] = None,
        max_posts_per_region: int = 100
    ):
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å production workflow
        
        Args:
            region_codes: –°–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ —Ä–µ–≥–∏–æ–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ)
            max_posts_per_region: –ú–∞–∫—Å–∏–º—É–º –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ —Ä–µ–≥–∏–æ–Ω
        """
        logger.info("\n" + "="*70)
        logger.info("üöÄ SETKA Production Workflow")
        logger.info("="*70)
        logger.info(f"Start time: {self.stats['start_time']}")
        
        try:
            # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –∑–∞–ø—É—Å–∫–µ workflow
            if region_codes:
                notify_workflow_started(region_codes)
            else:
                notify_workflow_started(["all_active_regions"])
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å VK —Ç–æ–∫–µ–Ω—ã
            vk_tokens = await self.get_vk_tokens()
            
            if not vk_tokens:
                logger.error("‚ùå No VK tokens available!")
                logger.error("üí° Run: python scripts/add_vk_tokens.py")
                return
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
            filters_data = await self.load_filters()
            
            # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–≥–∏–æ–Ω—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            async with AsyncSessionLocal() as session:
                if region_codes:
                    result = await session.execute(
                        select(Region).where(
                            and_(
                                Region.code.in_(region_codes),
                                Region.is_active == True
                            )
                        )
                    )
                else:
                    result = await session.execute(
                        select(Region).where(Region.is_active == True)
                    )
                
                regions = list(result.scalars().all())
            
            logger.info(f"\nüìä Will process {len(regions)} regions: {', '.join(r.code for r in regions)}")
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—ã–π —Ä–µ–≥–∏–æ–Ω
            all_region_stats = []
            
            for region in regions:
                region_stats = await self.process_region(
                    region_code=region.code,
                    vk_tokens=vk_tokens,
                    filters_data=filters_data,
                    max_posts=max_posts_per_region
                )
                
                all_region_stats.append(region_stats)
                
                self.stats['regions_processed'] += 1
                self.stats['posts_collected'] += region_stats['posts_collected']
                self.stats['posts_filtered'] += region_stats['posts_before_filter']
                self.stats['posts_accepted'] += region_stats['posts_accepted']
                self.stats['errors'].extend(region_stats['errors'])
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏
                await asyncio.sleep(2)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.stats['end_time'] = datetime.now()
            self.stats['duration'] = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            
            logger.info("\n" + "="*70)
            logger.info("üìä WORKFLOW COMPLETE - FINAL STATISTICS")
            logger.info("="*70)
            logger.info(f"Duration: {self.stats['duration']:.1f} seconds")
            logger.info(f"Regions processed: {self.stats['regions_processed']}")
            logger.info(f"Posts collected from VK: {self.stats['posts_collected']}")
            logger.info(f"Posts filtered: {self.stats['posts_filtered']}")
            logger.info(f"Posts accepted: {self.stats['posts_accepted']}")
            
            if self.stats['posts_filtered'] > 0:
                rejection_rate = 100 * (1 - self.stats['posts_accepted'] / self.stats['posts_filtered'])
                logger.info(f"Overall rejection rate: {rejection_rate:.1f}%")
            
            if self.stats['errors']:
                logger.warning(f"\n‚ö†Ô∏è Errors encountered: {len(self.stats['errors'])}")
                for error in self.stats['errors']:
                    logger.warning(f"  - {error}")
            
            logger.info("\n‚úÖ Production workflow completed successfully!")
            
            # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ workflow
            notify_workflow_completed(
                regions_processed=self.stats['regions_processed'],
                posts_collected=self.stats['posts_collected'],
                posts_accepted=self.stats['posts_accepted'],
                duration=self.stats['duration']
            )
            
        except Exception as e:
            logger.error(f"\n‚ùå Workflow failed: {str(e)}", exc_info=True)
            raise
    
    async def run_single_region(
        self,
        region_code: str,
        max_posts: int = 100,
        publish_mode: str = 'production'
    ) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ workflow –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ —Å –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π
        
        Args:
            region_code: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ (mi, nolinsk, etc)
            max_posts: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            publish_mode: 'test' –∏–ª–∏ 'production'
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–≥–∏–æ–Ω–∞
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"\nüåç Processing region: {region_code}")
            logger.info("="*60)
            
            # –ü–æ–ª—É—á–∏—Ç—å VK —Ç–æ–∫–µ–Ω—ã –∏ —Ñ–∏–ª—å—Ç—Ä—ã
            vk_tokens = await self.get_vk_tokens()
            filters_data = await self.load_filters()
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–≥–∏–æ–Ω
            region_stats = await self.process_region(
                region_code=region_code,
                vk_tokens=vk_tokens,
                filters_data=filters_data,
                max_posts=max_posts
            )
            
            # –ü–æ–ª—É—á–∏—Ç—å region_id –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            region_id = region_stats.get('region_id')
            
            # –î–æ–±–∞–≤–∏—Ç—å –ø—É–±–ª–∏–∫–∞—Ü–∏—é –¥–∞–π–¥–∂–µ—Å—Ç–∞
            posts_published = 0
            publish_error = None
            
            if region_stats['posts_accepted'] > 0:
                try:
                    # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    async with AsyncSessionLocal() as session:
                        result = await session.execute(
                            select(Post).where(
                                and_(
                                    Post.region_id == region_id,
                                    Post.ai_analyzed == True,
                                    Post.status == 'new',
                                    Post.date_published >= datetime.now() - timedelta(hours=24)
                                )
                            ).order_by(Post.ai_score.desc()).limit(5)
                        )
                        posts = result.scalars().all()
                    
                    if posts:
                        # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–∞—á–∞–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                        notify_publish_started(region_code, len(posts))
                        
                        # –°–æ–∑–¥–∞—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç
                        aggregator = NewsAggregator(max_posts_per_digest=5)
                        
                        # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–≥–∏–æ–Ω–µ
                        async with AsyncSessionLocal() as session:
                            result = await session.execute(
                                select(Region).where(Region.code == region_code)
                            )
                            region = result.scalar_one_or_none()
                        
                        if region:
                            title = f"üì∞ –ù–û–í–û–°–¢–ò {region.name.upper()}"
                            hashtags = [f"#–ù–æ–≤–æ—Å—Ç–∏{region_code.upper()}"]
                            
                            digest = await aggregator.aggregate(
                                posts=posts,
                                title=title,
                                hashtags=hashtags
                            )
                            
                            if digest:
                                # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ VK
                                from modules.publisher.vk_publisher import VKPublisher
                                from config.runtime import VK_MAIN_TOKENS
                                
                                publisher = VKPublisher(VK_MAIN_TOKENS["VALSTAN"]["token"])
                                target_group = publisher.get_target_group_id(region_code, publish_mode)
                                
                                publish_result = await publisher.publish_aggregated_post(
                                    digest, target_group
                                )
                                
                                if publish_result['success']:
                                    posts_published = 1
                                    logger.info(f"‚úÖ Published digest to VK: {publish_result['url']}")
                                    
                                    # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                                    notify_publish_completed(
                                        publish_result['post_id'],
                                        publish_result['url'],
                                        publish_result['group_id']
                                    )
                                else:
                                    publish_error = publish_result['error']
                                    logger.error(f"‚ùå Failed to publish: {publish_error}")
                                    
                                    # –û—Ç–ø—Ä–∞–≤–∏—Ç—å Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                                    try:
                                        from modules.notifications.telegram_notifier import get_telegram_notifier
                                        notifier = get_telegram_notifier()
                                        if notifier:
                                            await notifier.send_error_notification(
                                                f"Failed to publish digest for region {region_code}: {publish_error}",
                                                {'region_code': region_code, 'task_name': 'publish_digest'}
                                            )
                                    except Exception as e:
                                        logger.error(f"Failed to send Telegram notification: {e}")
                            else:
                                logger.warning("Failed to create digest")
                        else:
                            logger.error(f"Region {region_code} not found")
                    else:
                        logger.info("No posts available for publishing")
                        
                except Exception as e:
                    publish_error = str(e)
                    logger.error(f"Publishing failed: {e}", exc_info=True)
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            result = {
                'success': True,
                'region_code': region_code,
                'posts_collected': region_stats['posts_collected'],
                'posts_accepted': region_stats['posts_accepted'],
                'posts_published': posts_published,
                'duration': duration,
                'publish_mode': publish_mode,
                'errors': region_stats['errors']
            }
            
            if publish_error:
                result['publish_error'] = publish_error
                result['errors'].append(f"Publishing: {publish_error}")
            
            logger.info(f"\n‚úÖ Region {region_code} processing complete!")
            logger.info(f"   Posts collected: {region_stats['posts_collected']}")
            logger.info(f"   Posts accepted: {region_stats['posts_accepted']}")
            logger.info(f"   Posts published: {posts_published}")
            logger.info(f"   Duration: {duration:.1f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Failed to process region {region_code}: {e}", exc_info=True)
            return {
                'success': False,
                'region_code': region_code,
                'error': str(e),
                'duration': (datetime.now() - start_time).total_seconds()
            }


async def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SETKA Production Workflow')
    parser.add_argument(
        '--regions',
        nargs='+',
        help='Region codes to process (e.g., mi nolinsk). If not specified, all active regions will be processed.'
    )
    parser.add_argument(
        '--max-posts',
        type=int,
        default=100,
        help='Maximum posts per region to process (default: 100)'
    )
    
    args = parser.parse_args()
    
    workflow = ProductionWorkflow()
    await workflow.run(
        region_codes=args.regions,
        max_posts_per_region=args.max_posts
    )


if __name__ == "__main__":
    asyncio.run(main())

